### 注意力探索（14 分）

多头自注意力是变压器的核心建模组件。在这个问题中，我们将练习使用自注意力方程，并探讨为什么多头自注意力优于单头自注意力。

回顾一下，注意力可以视为对查询向量 \( q \in \mathbb{R}^d \)、一组值向量 \(\{v_1, \ldots, v_n\}\)、\(v_i \in \mathbb{R}^d\) 和一组键向量 \(\{k_1, \ldots, k_n\}\)、\(k_i \in \mathbb{R}^d\) 的操作，具体如下：

\[ c = \sum_{i=1}^{n} v_i \alpha_i \]

\[ \alpha_i = \frac{\exp(k_i^\top q)}{\sum_{j=1}^n \exp(k_j^\top q)} \]

其中 \(\alpha = \{\alpha_1, \ldots, \alpha_n\}\) 称为“注意力权重”。注意输出 \( c \in \mathbb{R}^d \) 是值向量的加权平均。

(a) **注意力中的复制**（3 分）：注意力的一个优势是它特别容易将值向量“复制”到输出 \(c\)。在这个问题中，我们将解释为什么会这样。

i. **（2 分）** 分布 \(\alpha\) 通常相对“分散”；概率质量分布在许多不同的 \(\alpha_i\) 之间。然而，这并不总是如此。描述（用一句话）在什么条件下，分类分布 \(\alpha\) 将几乎所有的权重放在某个 \(\alpha_j\) 上，其中 \(j \in \{1, \ldots, n\}\)（即 \(\alpha_j \gg \sum_{i \neq j} \alpha_i\)）。查询 \(q\) 和/或键 \(\{k_1, \ldots, k_n\}\) 必须满足什么条件？

ii. **（1 分）** 在您在（i）中给出的条件下，描述输出 \(c\)。

(b) **两个的平均值**（2 分）：与其只关注一个向量 \(v_j\)，变压器模型可能希望结合来自多个源向量的信息。

考虑我们希望结合来自两个向量 \(v_a\) 和 \(v_b\) 的信息，并且对应的键向量是 \(k_a\) 和 \(k_b\)。假设（1）所有键向量都是正交的，所以 \(k_i^\top k_j = 0\) 对于所有 \(i \neq j\)；（2）所有键向量的范数为 1。找到一个查询向量 \(q\) 的表达式，使得 \(c \approx \frac{1}{2}(v_a + v_b)\)，并证明您的答案。*（回顾您在第（a）部分中学到的内容。）

(c) **单头注意力的缺点**（5 分）：在前一部分中，我们看到了单头注意力如何可能等量地关注两个值。这个概念可以很容易地扩展到任何值的子集。在这个问题中，我们将看到为什么这不是一个实用的解决方案。

考虑一组键向量 \(\{k_1, \ldots, k_n\}\)，它们现在是随机采样的，\(k_i \sim \mathcal{N}(\mu_i, \Sigma_i)\)，其中均值 \(\mu_i \in \mathbb{R}^d\) 对您已知，但协方差 \(\Sigma_i\) 对您未知（除非问题中另有说明）。此外，假设均值 \(\mu_i\) 全部垂直；\(\mu_i^\top \mu_j = 0\) 如果 \(i \neq j\)，且单位范数，\(\|\mu_i\| = 1\)。

i. **（2 分）** 假设协方差矩阵为 \(\Sigma_i = \alpha I\)，对于所有 \(i \in \{1, 2, \ldots, n\}\)，\(\alpha\) 非常小。设计一个查询 \(q\) 用于 \(\mu_i\)，使得如前所述，\(c \approx \frac{1}{2}(v_a + v_b)\)，并提供一个简要论证。

ii. **（3 分）** 尽管单头注意力对键的小扰动具有抵抗力，但某些类型的大扰动可能会带来更大问题。在某些情况下，一个键向量 \(k_a\) 可能比其他的键向量大或小，尽管仍然指向与 \(\mu_a\) 相同的方向。

在这种情况下，假设一个项目的协方差为 \(\Sigma_a = \alpha I + \frac{1}{2}(\mu_a \mu_a^\top)\)，\(\alpha\) 非常小。这使得 \(k_a\) 大致指向与 \(\mu_a\) 相同的方向，但大小变化很大。此外，假设 \(\Sigma_i = \alpha I\) 对于所有 \(i \neq a\)。

当您多次采样 \(\{k_1, \ldots, k_n\}\)，并使用在第 (i) 部分定义的 \(q\) 向量时，您期望不同样本的 \(c\) 向量在质量上会是什么样子？考虑它与第 (i) 部分的不同之处以及 \(c\) 的方差将如何受到影响。

(d) **多头注意力的好处**（3 分）：现在我们将看到多头注意力的一些强大之处。我们将考虑一种简单版本的多头注意力，它与我们介绍的单头自注意力相同，除了定义了两个查询向量（\(q_1\) 和 \(q_2\)），这会产生一对向量（\(c_1\) 和 \(c_2\)），每个都是给定其各自查询向量的单头注意力的输出。多头注意力的最终输出是它们的平均值 \(\frac{1}{2}(c_1 + c_2)\)。

如问题 1(c) 所述，考虑一组键向量 \(\{k_1, \ldots, k_n\}\)，它们是随机采样的，\(k_i \sim \mathcal{N}(\mu_i, \Sigma_i)\)，均值 \(\mu_i\) 对您已知，但协方差 \(\Sigma_i\) 对您未知。与之前一样，假设均值 \(\mu_i\) 相互正交；\(\mu_i^\top \mu_j = 0\) 如果 \(i \neq j\)，且单位范数，\(\|\mu_i\| = 1\)。

i. **（1 分）** 假设协方差矩阵为 \(\Sigma_i = \alpha I\)，\(\alpha\) 非常小。设计 \(q_1\) 和 \(q_2\) 用于 \(\mu_i\)，使得 \(c\) 约等于 \(\frac{1}{2}(v_a + v_b)\)。注意 \(q_1\) 和 \(q_2\) 应具有不同的表达式。

ii. **（2 分）** 假设协方差矩阵为 \(\Sigma_a = \alpha I + \frac{1}{2}(\mu_a \mu_a^\top)\)，\(\alpha\) 非常小，且 \(\Sigma_i = \alpha I\) 对所有 \(i \neq a\)。使用在第 (i) 部分设计的查询向量 \(q_1\) 和 \(q_2\)，对于不同样本的键向量，您期望输出 \(c\) 在质量上会是什么样子？简要解释 \(c_1\) 和 \(c_2\) 的方差。

(e) **（1 分）** 基于第 (d) 部分，简要总结多头注意力如何克服您在第 (c) 部分中识别的单头注意力的缺点。





### 2. 位置嵌入探索（6 分）

位置嵌入是变压器架构的重要组成部分，使模型能够根据序列中的位置区分标记。在这个问题中，我们将探讨位置嵌入在变压器中的必要性以及它们如何设计。

回顾一下，变压器架构的关键组件是自注意力层和前馈神经网络层。给定输入张量 \(X \in \mathbb{R}^{T \times d}\)，其中 \(T\) 是序列长度，\(d\) 是隐藏维度，自注意力层计算如下：

\[Q = XW_Q, K = XW_K, V = XW_V\]

\[H = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V\]

其中 \(W_Q, W_K, W_V \in \mathbb{R}^{d \times d}\) 是权重矩阵，\(H \in \mathbb{R}^{T \times d}\) 是输出。

接下来，前馈层应用以下变换：

\[Z = \text{ReLU}(HW_1 + \mathbf{1} \cdot b_1)W_2 + \mathbf{1} \cdot b_2\]

其中 \(W_1, W_2 \in \mathbb{R}^{d \times d}\) 和 \(b_1, b_2 \in \mathbb{R}^{1 \times d}\) 是权重和偏置；\(\mathbf{1} \in \mathbb{R}^{T \times 1}\) 是一个全 1 的向量；\(Z \in \mathbb{R}^{T \times d}\) 是最终输出。

（注意：为了简化，我们省略了变压器架构的一些细节。）

(a) **置换输入**。

i. **（3 分）** 假设我们置换输入序列 \(X\)，使得标记被随机打乱。这可以表示为乘以置换矩阵 \(P \in \mathbb{R}^{T \times T}\)，即 \(X_{\text{perm}} = PX\)。证明置换输入 \(X_{\text{perm}}\) 的输出 \(Z_{\text{perm}}\) 将是 \(Z_{\text{perm}} = PZ\)。

给定任何置换矩阵 \(P\) 和任意矩阵 \(A\)，以下恒成立：

\[\text{softmax}(PAP^\top) = P \text{softmax}(A) P^\top\]  
\[\text{ReLU}(PA) = P \text{ReLU}(A)\]

ii. **（1 分）** 思考您在第 (i) 部分中推导出的结果的含义。解释为什么变压器模型的这个特性在处理文本时可能会有问题。

(b) **位置嵌入是根据序列中每个标记的位置编码的向量**。它们在将输入词嵌入输入变压器之前被添加到输入词嵌入中。一种方法是使用位置和嵌入维度的固定函数生成位置嵌入。如果输入词嵌入是 \(X \in \mathbb{R}^{T \times d}\)，位置嵌入 \(\Phi \in \mathbb{R}^{T \times d}\) 生成如下：

\[
\Phi(t, 2i) = \sin \left(\frac{t}{10000^{2i/d}}\right) \\
\Phi(t, 2i+1) = \cos \left(\frac{t}{10000^{2i/d}}\right)
\]

其中 \(t \in \{0, 1, \ldots T-1\}\) 和 \(i \in \{0, 1, \ldots d/2-1\}\)。

具体来说，位置嵌入被添加到输入词嵌入中：

\[X_{\text{pos}} = X + \Phi\]

i. **（1 分）** 您认为位置嵌入是否有助于解决您在第 (a) 部分中识别的问题？如果是，请解释如何解决；如果不是，请解释为什么不解决。

ii. **（1 分）** 输入序列中两个不同标记的位置嵌入是否可能相同？如果是，请提供一个例子。如果不是，请解释为什么不可能。

### 3. 预训练的变压器模型和知识访问（35 分）

您将训练一个变压器执行一个涉及访问世界知识的任务——这些知识并未通过任务的训练数据提供（至少如果您想要超出训练集进行泛化）。您会发现它几乎完全无法完成任务。然后，您将学习如何在包含世界知识的维基百科文本上预训练该变压器，并发现微调该变压器在相同的知识密集型任务上能够访问一些预训练时学到的知识。您会发现这使得模型在保留的开发集上表现明显高于偶然水平。

提供给您的代码是 Andrej Karpathy 的 minGPT 的一个分支。它比大多数研究代码更简单和透明。minGPT 中的“GPT”指的是 OpenAI 的变压器语言模型，最初在这篇论文中描述 [1]。

正如之前的作业，您将希望在本地机器上进行开发，然后在 GCP/Colab 上进行训练。您可以使用之前作业中的相同 conda 环境进行本地开发，并使用相同的流程在 GPU 上进行训练。

您将需要大约 3 小时进行训练，因此请合理安排时间！我们提供了一个示例 Colab，其中包含需要 GPU 训练的命令。请注意，数据集多处理可能会在没有 GPU 的本地机器上失败，因此要在本地调试，您可能需要将 num_workers 更改为 0。

您使用此代码库的工作如下：

(a) **（0 分）** 查看演示。

在 mingpt-demo/ 文件夹中有一个 Jupyter 笔记本 play_char.ipynb，它训练并从变压器语言模型中采样。看看它（在您的计算机上本地），以便对它如何定义和训练模型有所了解。您在下面编写的一些代码将受此笔记本中所见内容的启发。

注意，您不需要为此部分编写任何代码、运行笔记本或提交书面答案。

(b) **（0 分）** 阅读 src/dataset.py 中的 NameDataset，我们用于读取名字-出生地对的数据集。

我们将使用预训练模型进行的任务是尝试访问名人在其维基百科页面上写的出生地。我们将其视为一种特别简单的问答形式：

问：某人 [x] 出生在哪里？  
答：某地

从现在开始，您将使用 src/ 文件夹中的内容。mingpt-demo/ 中的代码不会被更改或评估。在 dataset.py 中，您会找到 NameDataset 类，它读取一个包含名字/地点对的 TSV（制表符分隔值）文件，并生成我们可以馈送给变压器模型的上述形式的示例。

要了解我们将使用的示例，如果您运行以下代码，它将加载 NameDataset 上的训练集 birth_places_train.tsv 并打印出几个示例。

```python
python src/dataset.py namedata
```

注意，您不需要为此部分编写任何代码或提交书面答案。

(c) **（0 分）** 实现微调（无预训练）。

看看 run.py。它有一些骨架代码指定了您最终需要作为命令行参数处理的标志。特别是，您可能想要使用此代码进行预训练、微调或评估模型。现在，我们将专注于无预训练情况下的微调功能。

借鉴 play_char.ipynb 文件中的训练代码的灵感，编写代码以微调变压器模型在名字/出生地数据集上，通过 NameDataset 类的示例。现在，实现无预训练情况下的情况（即从头创建一个模型，并在第 (b) 部分的出生地预测任务上训练它）。您需要修改代码中标记为 [part c] 的两个部分：一个用于初始化模型，另一个用于微调它。请注意，现在您只需要初始化标记为“vanilla”的情况（在第 (g) 部分中，我们将探讨一个模型变体）。

还要看看为您实现的评估代码。它从训练的模型中采样预测，并调用 evaluate_places() 获取正确的地点预测的总百分比。您将在第 (d) 部分运行此代码以评估您的训练模型。

这是后续部分的中间步骤，包括第 (d) 部分，其中包含您可以运行的命令以检查您的实现。此部分不需要书面回答。

提示：run.py 和 play_char.ipynb 都使用 minGPT，因此此部分的代码将类似于 play_char.ipynb 中的训练代码。

(d) **（4 分）** 进行预测（无预训练）。

在 birth_places_train.tsv 上训练您的模型，并在 birth_dev.tsv 上进行评估。具体来说，您现在应该能够运行以下三个

命令：

```bash
# 在名字数据集上训练
python src/run.py finetune vanilla wiki.txt \
--writing_params_path vanilla.model.params \
--finetune_corpus_path birth_places_train.tsv

# 在开发集上评估，写出预测
python src/run.py evaluate vanilla wiki.txt \
--reading_params_path vanilla.model.params \
--eval_corpus_path birth_dev.tsv \
--outputs_path vanilla.nopretrain.dev.predictions

# 在测试集上评估，写出预测
python src/run.py evaluate vanilla wiki.txt \
--reading_params_path vanilla.model.params \
--eval_corpus_path birth_test_inputs.tsv \
--outputs_path vanilla.nopretrain.test.predictions
```

训练将花费不到 10 分钟（在 GCP 上）。报告您的模型在开发集上的准确性（如上面第二个命令所打印）。类似于作业 3，我们在作业 4 中也有 Tensorboard 日志记录用于调试。它可以使用 `tensorboard --logdir expt/` 启动。如果准确性远低于 10%，请不要惊讶；我们将在第 4 部分深入研究原因。作为参考点，我们还希望计算模型如果仅预测“伦敦”作为开发集中每个人的出生地将获得的准确性。填写 `london_baseline.py` 以计算该方法的准确性，并在文件中报告结果。您应该能够利用现有代码，使该文件只有几行长。

(e) **（10 分）** 定义一个用于预训练的跨度损坏函数。

在文件 src/dataset.py 中，实现 CharCorruptionDataset 类的 `__getitem__()` 函数。按照 dataset.py 中注释提供的说明进行实现。跨度损坏在 T5 论文 [2] 中进行了探索。它随机选择文档中的文本跨度，并将它们替换为唯一标记（噪声化）。模型获取这些噪声文本，并需要输出输入中由该标记替换的每个唯一标记后的标记模式。在这个问题中，您将实现一个简化版本，只遮蔽一段字符序列。

此问题将通过自动评分器根据您的跨度损坏函数是否实现了我们规范的一些基本属性进行评分。我们将用我们自己的数据实例化 CharCorruptionDataset，并从中抽取示例。

为帮助您调试，如果您运行以下代码，它将在预训练数据集 wiki.txt 上从 CharCorruptionDataset 中抽取一些示例并打印出来。

```python
python src/dataset.py charcorruption
```

(f) **（10 分）** 预训练、微调和进行预测。预留大约 1 小时进行训练。

现在填写 run.py 的预训练部分，该部分将在跨度损坏任务上预训练一个模型。此外，修改您的微调部分以处理预训练情况下的微调。特别是，如果在 bash 命令中提供了预训练模型的路径，请在微调出生地预测任务之前加载该模型。在 wiki.txt 上预训练您的模型（大约需要 40-60 分钟），在 NameDataset 上微调并评估它。具体来说，您应该能够运行以下四个命令：

```bash
# 预训练模型
python src/run.py pretrain vanilla wiki.txt \
--writing_params_path vanilla.pretrain.params

# 微调模型
python src/run.py finetune vanilla wiki.txt \
--reading_params_path vanilla.pretrain.params \
--writing_params_path vanilla.finetune.params \
--finetune_corpus_path birth_places_train.tsv

# 在开发集上评估；写入磁盘
python src/run.py evaluate vanilla wiki.txt \
--reading_params_path vanilla.finetune.params \
--eval_corpus_path birth_dev.tsv \
--outputs_path vanilla.pretrain.dev.predictions

# 在测试集上评估；写入磁盘
python src/run.py evaluate vanilla wiki.txt \
--reading_params_path vanilla.finetune.params \
--eval_corpus_path birth_test_inputs.tsv \
--outputs_path vanilla.pretrain.test.predictions
```

我们期望开发集上的准确性至少为 15%，并期望在保留的测试集上有类似的准确性。

(g) **（11 分）** 编写并尝试不同类型的位置嵌入（预留大约 1 小时进行训练）

在上一部分中，您使用了 vanilla 变压器模型，它使用了学习到的位置嵌入。在书面部分，您还了解了原始变压器论文中使用的正弦位置嵌入。在这一部分中，您将实现一种不同类型的位置嵌入，称为 RoPE（旋转位置嵌入）[3]。

RoPE 是一种固定位置嵌入，旨在编码相对位置而非绝对位置。绝对位置的问题在于，如果变压器在训练时使用较短的上下文长度（例如 128），在更大的上下文长度（例如 1000）上表现不佳，因为位置嵌入的分布与训练时的分布会有很大不同。相对位置嵌入如 RoPE 可以缓解这一问题。

给定在序列位置 t 上的两个特征 \(x_t^{(1)}\) 和 \(x_t^{(2)}\)，RoPE 位置嵌入定义如下：

\[
\text{RoPE}(x_t^{(1)}, x_t^{(2)}, t) = \begin{pmatrix}
\cos t\theta & -\sin t\theta \\
\sin t\theta & \cos t\theta
\end{pmatrix} \begin{pmatrix}
x_t^{(1)} \\
x_t^{(2)}
\end{pmatrix}
\]

其中 \(\theta\) 是一个固定角度。对于两个特征，RoPE 操作对应于按角度 \(t\theta\) 旋转特征。注意该角度是位置 \(t\) 的函数。

对于 \(d\) 维特征，RoPE 应用于每对特征，角度 \(\theta_i\) 定义为：

\[
\theta_i = 10000^{-2(i-1)/d}, \quad i \in \{1, 2, \ldots, d/2\}
\]

\[
\begin{pmatrix}
\cos t\theta_1 & -\sin t\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
\sin t\theta_1 & \cos t\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos t\theta_2 & -\sin t\theta_2 & \cdots & 0 & 0 \\
0 & 0 & \sin t\theta_2 & \cos t\theta_2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos t\theta_{d/2} & -\sin t\theta_{d/2} \\
0 & 0 & 0 & 0 & \cdots & \sin t\theta_{d/2} & \cos t\theta_{d/2}
\end{pmatrix}
\begin{pmatrix}
x_t^{(1)} \\
x_t^{(2)} \\
x_t^{(3)} \\
x_t^{(4)} \\
\vdots \\
x_t^{(d-1)} \\
x_t^{(d)}
\end{pmatrix}
\]

最后，RoPE 被应用于每个注意力块中每个头的键和查询向量，而不是将位置嵌入添加到输入嵌入中。

i. **（2 分）** 使用旋转解释，RoPE 操作可以视为将复数 \(x_t^{(1)} + ix_t^{(2)}\) 按角度 \(t\theta\) 旋转。回顾这对应于乘以 \(e^{it\theta} = \cos t\theta + i\sin t\theta\)。

对于更高维特征向量，此解释允许我们更有效地计算公式 3。具体来说，我们可以将 RoPE 操作重新表示为两个向量的元素级乘积（记作 \(\odot\)）如下：

\[
\begin{pmatrix}
\cos t\theta_1 + i\sin t\theta_1 \\
\cos t\theta_2 + i\sin t\theta_2 \\
\vdots \\
\cos t\theta_{d/2} + i\sin t\theta_{d/2}
\end{pmatrix} \odot
\begin{pmatrix}
x_t^{(1)} + ix_t^{(2)} \\
x_t^{(3)} + ix_t^{(4)} \\
\vdots \\
x_t^{(d-1)} + ix_t^{(d)}
\end{pmatrix}
\]

显示方程 3 中的元素可以通过方程 4 获得。注意，一些额外的操作如重塑是必要的以使两个表达式相等，但您不需要提供详细的

推导即可获得满分。

ii. **（1 分）** 相对嵌入。现在我们将显示 RoPE 嵌入的两个向量在位置 \(t_1\) 和 \(t_2\) 的点积仅取决于相对位置 \(t_1 - t_2\)。为了简化，我们将假设二维特征向量（例如 \([a, b]\)），并使用它们的复数表示（例如 \(a + ib\)）。

显示 \(\langle \text{RoPE}(z_1, t_1), \text{RoPE}(z_2, t_2) \rangle = \langle \text{RoPE}(z_1, t_1 - t_2), \text{RoPE}(z_2, 0) \rangle\)，其中 \(\langle \cdot, \cdot \rangle\) 表示点积，\(\text{RoPE}(z, t)\) 是位置 \(t\) 处向量 \(z\) 的 RoPE 嵌入。

（提示：以复数表示的向量点积由 \(\langle z_1, z_2 \rangle = \text{Re}(z_1 z_2)\) 给出。对于复数 \(z = a + ib\)（\(a, b \in \mathbb{R}\)），\(\text{Re}(z) = a\) 表示 \(z\) 的实部，\(z̄ = a - ib\) 是 \(z\) 的共轭复数。）

iii. **（8 分）** 提供的代码中，RoPE 通过 src/attention.py 中的 precompute_rotary_emb 和 apply_rotary_emb 函数实现。您需要实现这些函数和 src/attention.py 和 src/run.py 中标记为 [part g] 的代码部分以在模型中使用 RoPE。

在跨度损坏任务上训练带有 RoPE 的模型，并在出生地预测任务上微调它。具体来说，您应该能够运行以下四个命令：

```bash
# 预训练模型
python src/run.py pretrain rope wiki.txt \
--writing_params_path rope.pretrain.params

# 微调模型
python src/run.py finetune rope wiki.txt \
--reading_params_path rope.pretrain.params \
--writing_params_path rope.finetune.params \
--finetune_corpus_path birth_places_train.tsv

# 在开发集上评估；写入磁盘
python src/run.py evaluate rope wiki.txt \
--reading_params_path rope.finetune.params \
--eval_corpus_path birth_dev.tsv \
--outputs_path rope.pretrain.dev.predictions

# 在测试集上评估；写入磁盘
python src/run.py evaluate rope wiki.txt \
--reading_params_path rope.finetune.params \
--eval_corpus_path birth_test_inputs.tsv \
--outputs_path rope.pretrain.test.predictions
```

我们将对您的模型进行评分，查看它是否在保留的测试集上获得至少 30% 的准确率。

### 4. 预训练知识的考虑（5 分）

请将这些书面问题的答案输入到计算机中（以

方便助教评分）。

(a) **（1 分）** 简洁地解释为什么预训练的（vanilla）模型能够达到 10%以上的准确率，而未预训练的模型则不能。

(b) **（2 分）** 看看预训练+微调的 vanilla 模型的一些正确预测和一些错误预测。我们认为您会发现，仅看输出，无法判断模型是检索到了正确的出生地，还是捏造了一个错误的出生地。考虑这种行为对包含预训练 NLP 组件的用户系统的影响。提出两个不同的原因，说明这种模型行为（即无法判断是检索还是捏造）可能会对这些应用程序造成担忧，并为每个原因提供一个示例。

(c) **（2 分）** 如果您的模型在预训练时未见过某个人的名字，并且在微调时也未见过这个人，它不可能“知道”他们的居住地。然而，如果被问到，该模型仍会给出一个预测的出生地。简要描述您的模型可能采取的策略来预测该人的出生地，并说明这种应用方式可能带来的一个伦理问题。

（虽然第 4b 部分讨论了捏造预测可能带来的问题，但第 4c 部分要求提供一种机制，说明模型在微调时未见过某人名字的情况下是如何生成该人的出生地预测的，并解释这种机制为什么会带来问题。）

### 提交说明

您将以两次提交的方式在 GradeScope 上提交此作业——一次是 Assignment 4 [coding]，另一次是 Assignment 4 [written]：

1. 验证以下文件是否在您的作业目录中的指定路径存在：

- 无预训练的模型和预测：vanilla.model.params，vanilla.nopretrain.dev.predictions，vanilla.nopretrain.test.predictions
- 伦敦基线准确性：london_baseline_accuracy.txt
- 预训练-微调模型和预测：vanilla.finetune.params，vanilla.pretrain.dev.predictions，vanilla.pretrain.test.predictions
- RoPE 模型和预测：rope.finetune.params，rope.pretrain.dev.predictions，rope.pretrain.test.predictions

2. 运行 collect_submission.sh（在 Linux/Mac 上）或 collect_submission.bat（在 Windows 上）以生成您的 assignment4.zip 文件。

3. 将您的 assignment4.zip 文件上传到 GradeScope 到 Assignment 4 [coding]。

4. 检查公共自动评分测试是否正确通过。

5. 将您的书面解决方案（问题 1、第 2 部分和第 3 部分的部分）上传到 GradeScope 到 Assignment 4 [written]。正确标记它们！

### 参考文献

[1] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding with unsupervised learning. Technical report, OpenAI (2018).

[2] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research 21, 140 (2020), 1–67.

[3] Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063.

---

这个文档的翻译已经完成，请检查并继续进行接下来的步骤。如果您需要更多帮助，请告诉我。